{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corrective Retrieval-Augmented Generation (CRAG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting a user-agent environment variable for web requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"USER_AGENT\"] = \"CRAG/1.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, List\n",
    "from typing_extensions import TypedDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing required LangChain modules for handling documents, prompts, and messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_openai import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing graph utilities from LangGraph for workflow automation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.checkpoint.memory import MemorySaver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enums and Pydantic models for structured document grading response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class RAGDocumentGrade(Enum):\n",
    "    relevant = \"relevant\"\n",
    "    irrelevant = \"irrelevant\"\n",
    "\n",
    "\n",
    "class RAGDocumentGraderResponse(BaseModel):\n",
    "    \"\"\"Represents the structured response for grading a retrieved document in a RAG-based system.\"\"\"\n",
    "    grade: RAGDocumentGrade = Field(\n",
    "        ..., description=\"\"\"The assigned grade indicating whether the document is relevant (\"relevant\") or not (\"irrelevant\")\"\"\"\n",
    "    )\n",
    "    description: str | None = Field(\n",
    "        None, description=\"\"\"Additional context or reasoning for the grading, typically provided when the grade is \"irrelevant\". This field is optional.\"\"\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the graph state to hold information during execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CRAGState(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    question: str\n",
    "    answer: str\n",
    "    document_grader_response: RAGDocumentGraderResponse\n",
    "    crawler_response: str\n",
    "    rag_context: List[Document]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up memory for tracking graph execution, LLM and embedding function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = MemorySaver()\n",
    "\n",
    "llm = init_chat_model(model=\"gpt-4o-mini\", model_provider=\"openai\")\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining tools for crawler_agent and binding them to a LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool, Tool\n",
    "from langchain_community.tools import (\n",
    "    DuckDuckGoSearchResults,\n",
    "    WikipediaQueryRun,\n",
    "    YouTubeSearchTool,\n",
    ")\n",
    "from langchain_community.utilities import (\n",
    "    DuckDuckGoSearchAPIWrapper,\n",
    "    WikipediaAPIWrapper,\n",
    ")\n",
    "from langchain_community.tools.wikidata.tool import WikidataAPIWrapper, WikidataQueryRun\n",
    "from langchain_community.tools.yahoo_finance_news import YahooFinanceNewsTool\n",
    "\n",
    "\n",
    "def get_news_search_tool(region=\"in-en\"):\n",
    "    news_search_tool_wrapper = DuckDuckGoSearchAPIWrapper(\n",
    "        region=region,\n",
    "        time=\"d\",\n",
    "        max_results=5,\n",
    "    )\n",
    "    news_search_tool = Tool(\n",
    "        name=\"latest_news_search\",\n",
    "        description=\"Useful for searching latest news articles.\",\n",
    "        func=DuckDuckGoSearchResults(\n",
    "            api_wrapper=news_search_tool_wrapper,\n",
    "            source=\"news\",\n",
    "        ).run,\n",
    "    )\n",
    "    return news_search_tool\n",
    "\n",
    "\n",
    "def get_web_search_tool():\n",
    "    news_search_tool = Tool(\n",
    "        name=\"web_search\",\n",
    "        description=\"Useful for searching the web.\",\n",
    "        func=DuckDuckGoSearchResults().run,\n",
    "    )\n",
    "    return news_search_tool\n",
    "\n",
    "\n",
    "wikipedia_tool = Tool(\n",
    "    name=\"wikipedia_search\",\n",
    "    description=\"Useful for searching on Wikipedia.\",\n",
    "    func=WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper()).run,\n",
    ")\n",
    "\n",
    "\n",
    "wikidata_tool = Tool(\n",
    "    name=\"wikidata_search\",\n",
    "    description=\"Useful for searching on Wikidata.\",\n",
    "    func=WikidataQueryRun(api_wrapper=WikidataAPIWrapper()).run,\n",
    ")\n",
    "\n",
    "\n",
    "youtube_search_tool = Tool(\n",
    "    name=\"youtube_search\",\n",
    "    description=\"Useful for searching on youtube.\",\n",
    "    func=YouTubeSearchTool().run,\n",
    ")\n",
    "\n",
    "\n",
    "tools = [\n",
    "    get_web_search_tool(), wikipedia_tool, wikidata_tool, youtube_search_tool, get_news_search_tool()\n",
    "]\n",
    "llm_with_tools = llm.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up Chroma DB for vector storage and retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "\n",
    "\n",
    "chroma_db_collection_name = \"rag_db\"\n",
    "chroma_db_path = f\"./{chroma_db_collection_name}\"\n",
    "vector_store = Chroma(\n",
    "    collection_name=chroma_db_collection_name,\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory=chroma_db_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions to extract text from PDFs and store them in Chroma DB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "def data_extracter(file_path):\n",
    "    loader = PyPDFLoader(file_path)\n",
    "    pages: List[Document] = []\n",
    "    for page in loader.lazy_load():\n",
    "        pages.append(page)\n",
    "    return pages\n",
    "\n",
    "\n",
    "def split_text(pages: List, chunk_size=1000, chunk_overlap=200):\n",
    "    recursive_text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size, chunk_overlap=chunk_overlap\n",
    "    )\n",
    "    splits = recursive_text_splitter.split_documents(pages)\n",
    "    return splits\n",
    "\n",
    "\n",
    "def add_documents_to_db(docs: List[Document]):\n",
    "    document_ids = vector_store.add_documents(documents=docs)\n",
    "    return document_ids\n",
    "\n",
    "\n",
    "def add_document(file_path: str):\n",
    "    pages = data_extracter(file_path)\n",
    "    splits = split_text(pages)\n",
    "    add_documents_to_db(splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding some documents to Chroma DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded: files/attention-is-all-you-need-Paper.pdf\n",
      "Uploaded: files/rag.pdf\n",
      "Uploaded: files/self rag.pdf\n",
      "Uploaded: files/corrective_rag.pdf\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "folder_path = Path(\"files\")\n",
    "filenames = [f.name for f in folder_path.iterdir() if f.is_file()]\n",
    "for f in filenames:\n",
    "    rel_path = folder_path.joinpath(f)\n",
    "    add_document(rel_path)\n",
    "    print(\"Uploaded:\", rel_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph's Node functions\n",
    "\n",
    "Function to retrieve relevant documents from Chroma DB based on user query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_retriver(state: CRAGState):\n",
    "    messages = state.get(\"messages\", [])\n",
    "    last_user_message = None\n",
    "    n = len(messages)\n",
    "    for i in range(n-1, -1, -1):\n",
    "        msg = messages[i]\n",
    "        if isinstance(msg, HumanMessage):\n",
    "            last_user_message = msg.content\n",
    "            break\n",
    "    if last_user_message is None:\n",
    "        raise Exception(\"No user message found in the conversation.\")\n",
    "    retrieved_docs = vector_store.similarity_search(last_user_message)\n",
    "    context = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)\n",
    "    \n",
    "    new_state = CRAGState(**state)\n",
    "    new_state[\"rag_context\"] = retrieved_docs\n",
    "    new_state[\"question\"] = last_user_message\n",
    "    new_state[\"messages\"] = [{\"content\": context, \"role\": \"ai\"}]\n",
    "    return new_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to grade retrieved documents for relevance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_grader(state: CRAGState):\n",
    "    question = state.get(\"question\")\n",
    "    context = state.get(\"rag_context\", [])\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in context)\n",
    "\n",
    "    grader_llm = llm.with_structured_output(RAGDocumentGraderResponse)\n",
    "    grader_prompt_template = ChatPromptTemplate([\n",
    "        (\"system\", \"You are an expert evaluator responsible for grading retrieved documents in a Retrieval Augmented Generation (RAG) system. Your task is to assess whether the retrieved context is relevant and useful in answering the question or not, also give a proper reason if the context in not relevant.\"),\n",
    "        (\"human\", \"question: {question}\\ncontext: {context}\")\n",
    "    ])\n",
    "    chain = grader_prompt_template | grader_llm\n",
    "    grader_response = chain.invoke({\"question\": question, \"context\": docs_content})\n",
    "\n",
    "    new_state = CRAGState(**state)\n",
    "    new_state[\"document_grader_response\"] = grader_response\n",
    "    new_state[\"messages\"] = [{\"content\": grader_response.model_dump_json(), \"role\": \"ai\"}]\n",
    "    return new_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to rephrase query for web search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rephrase_query(state: CRAGState):\n",
    "    question = state.get(\"question\")\n",
    "    prompt_template = ChatPromptTemplate([\n",
    "        (\"system\", \"You are an expert in query optimization and search enhancement. Your task is to rephrase and improve user query to make them clearer, more specific, and better suited for retrieval in a search engine or a Retrieval Augmented Generation (RAG) system.\"),\n",
    "        (\"human\", \"Question: {question}\\nRephrased Question:\")\n",
    "    ])\n",
    "    chain = prompt_template | llm | StrOutputParser()\n",
    "    rephrased_question = chain.invoke({\"question\": question})\n",
    "\n",
    "    new_state = CRAGState(**state)\n",
    "    new_state[\"question\"] = rephrased_question\n",
    "    new_state[\"messages\"] = [{\"content\": rephrased_question, \"role\": \"ai\"}]\n",
    "    return new_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to search for answer using various tools like wikipedia, web search etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawler_agent(state: CRAGState):\n",
    "    if state.get(\"crawler_response\", None) is None:\n",
    "        llm_input = state.get(\"question\")\n",
    "    else:\n",
    "        llm_input = state.get(\"messages\", [])\n",
    "    \n",
    "    response = llm_with_tools.invoke(llm_input)\n",
    "\n",
    "    new_state = CRAGState(**state)\n",
    "    new_state[\"crawler_response\"] = response.content\n",
    "    new_state[\"messages\"] = [response]\n",
    "    return new_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to finally respond to the user using the data collected till now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def responder(state: CRAGState):\n",
    "    question = state.get(\"question\")\n",
    "    final_context = None\n",
    "    should_consider_rag_context = False\n",
    "    document_grader_response = state.get(\"document_grader_response\", None)\n",
    "    if document_grader_response is None or document_grader_response.grade == \"relevant\":\n",
    "        should_consider_rag_context = True\n",
    "    if should_consider_rag_context:\n",
    "        context = state.get(\"rag_context\", [])\n",
    "        docs_content = \"\\n\\n\".join(doc.page_content for doc in context)\n",
    "        final_context = docs_content\n",
    "    else:\n",
    "        final_context = state.get(\"crawler_response\", \"No Context Found\")\n",
    "    \n",
    "    prompt_template = ChatPromptTemplate(\n",
    "        [\n",
    "            (\n",
    "                \"system\",\n",
    "                \"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\",\n",
    "            ),\n",
    "            (\"user\", \"Question: {question}\\nContext: {context}\\nAnswer:\"),\n",
    "        ]\n",
    "    )\n",
    "    chain = prompt_template | llm | StrOutputParser()\n",
    "    answer = chain.invoke({\"question\": question, \"context\": final_context})\n",
    "\n",
    "    new_state = CRAGState(**state)\n",
    "    new_state[\"answer\"] = answer\n",
    "    new_state[\"messages\"] = [{\"content\": answer, \"role\": \"assistant\"}]\n",
    "    return new_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a graph to structure the RAG pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "\n",
    "def document_grader_route_condition(state: CRAGState) -> Literal[\"rephrase_query\", \"responder\"]:\n",
    "    document_grader_response = state.get(\"document_grader_response\", None)\n",
    "    if document_grader_response is None or document_grader_response.grade == \"relevant\":\n",
    "        return \"responder\"\n",
    "    return \"rephrase_query\"\n",
    "\n",
    "\n",
    "def custom_tools_condition(\n",
    "    state: CRAGState,\n",
    "    messages_key: str = \"messages\",\n",
    ") -> Literal[\"tools\", \"responder\"]:\n",
    "    if isinstance(state, list):\n",
    "        ai_message = state[-1]\n",
    "    elif isinstance(state, dict) and (messages := state.get(messages_key, [])):\n",
    "        ai_message = messages[-1]\n",
    "    elif messages := getattr(state, messages_key, []):\n",
    "        ai_message = messages[-1]\n",
    "    else:\n",
    "        raise ValueError(f\"No messages found in input state to tool_edge: {state}\")\n",
    "    if hasattr(ai_message, \"tool_calls\") and len(ai_message.tool_calls) > 0:\n",
    "        return \"tools\"\n",
    "    return \"responder\"\n",
    "\n",
    "\n",
    "graph_builder = StateGraph(CRAGState)\n",
    "graph_builder.add_sequence([rag_retriver, document_grader])\n",
    "graph_builder.add_node(\"rephrase_query\", rephrase_query)\n",
    "graph_builder.add_node(\"crawler_agent\", crawler_agent)\n",
    "graph_builder.add_node(\"responder\", responder)\n",
    "\n",
    "tool_node = ToolNode(tools=tools)\n",
    "graph_builder.add_node(\"tools\", tool_node)\n",
    "\n",
    "graph_builder.set_entry_point(\"rag_retriver\")\n",
    "\n",
    "graph_builder.add_conditional_edges(\"document_grader\", document_grader_route_condition, path_map={\n",
    "    \"rephrase_query\": \"rephrase_query\",\n",
    "    \"responder\": \"responder\",\n",
    "})\n",
    "graph_builder.add_edge(\"rephrase_query\", \"crawler_agent\")\n",
    "\n",
    "graph_builder.add_conditional_edges(\"crawler_agent\", custom_tools_condition, {\"tools\": \"tools\", \"responder\": \"responder\"})\n",
    "graph_builder.add_edge(\"tools\", \"crawler_agent\")\n",
    "\n",
    "\n",
    "graph_builder.add_edge(\"responder\", END)\n",
    "\n",
    "graph = graph_builder.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drawing graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'---\\nconfig:\\n  flowchart:\\n    curve: linear\\n---\\ngraph TD;\\n\\t__start__([<p>__start__</p>]):::first\\n\\trag_retriver(rag_retriver)\\n\\tdocument_grader(document_grader)\\n\\trephrase_query(rephrase_query)\\n\\tcrawler_agent(crawler_agent)\\n\\tresponder(responder)\\n\\ttools(tools)\\n\\t__end__([<p>__end__</p>]):::last\\n\\t__start__ --> rag_retriver;\\n\\trag_retriver --> document_grader;\\n\\trephrase_query --> crawler_agent;\\n\\tresponder --> __end__;\\n\\ttools --> crawler_agent;\\n\\tdocument_grader -.-> rephrase_query;\\n\\tdocument_grader -.-> responder;\\n\\tcrawler_agent -.-> tools;\\n\\tcrawler_agent -.-> responder;\\n\\tclassDef default fill:#f2f0ff,line-height:1.2\\n\\tclassDef first fill-opacity:0\\n\\tclassDef last fill:#bfb6fc\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.get_graph().draw_mermaid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inferencing - Executing the RAG workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"1\"}, \"recursion_limit\": 25}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "what is the name of twitter's ceo?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "from labor, stating: We find a good description of this in sources such as the book Rest Days which states:\n",
      "Early Christian observance of both the spiritual seventh-day sabbath and a Lord´s Day assembly is evidenced in\n",
      "Ignatius´s letter to the Magnesians ca. 110.\n",
      "Reference [2] Despite the official adoption of Sunday as a day of rest by Constantine, the seven-day week and\n",
      "the nundial cycle continued to be used side-by-side until at least the Calendar of 354 and probably later. ... The\n",
      "fact that the canon had to be issued at all is an indication that adoption of Constantine’s decree of 321 was still\n",
      "not universal ...\n",
      "Input Tell me a bio about Rory Tapner. (bio generation)\n",
      "Output <p>Reference [1]<p>[ ISREL =Relevant] Rory Tapner is a British businessman who has served as\n",
      "the Chief Executive Officer of Coutts, a private banking and wealth management company, since 2010.[ ISSUP\n",
      "=Contradictory], Reference [2]<p>[ ISREL =Relevant] Rory Tapner was born on 30 September 1959\n",
      "\n",
      "Sunipa Dev, Henryk Michalewski, Xavier Garcia,\n",
      "Vedant Misra, Kevin Robinson, Liam Fedus, Denny\n",
      "Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim,\n",
      "Barret Zoph, Alexander Spiridonov, Ryan Sepassi,\n",
      "David Dohan, Shivani Agrawal, Mark Omernick,\n",
      "Andrew M. Dai, Thanumalayan Sankaranarayana\n",
      "Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira,\n",
      "Rewon Child, Oleksandr Polozov, Katherine Lee,\n",
      "Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark\n",
      "Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy\n",
      "Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov,\n",
      "and Noah Fiedel. 2023. Palm: Scaling language\n",
      "modeling with pathways. J. Mach. Learn. Res. ,\n",
      "24:240:1–240:113.\n",
      "Shehzaad Dhuliawala, Mojtaba Komeili, Jing Xu,\n",
      "Roberta Raileanu, Xian Li, Asli Celikyilmaz, and\n",
      "Jason Weston. 2024. Chain-of-verification reduces\n",
      "hallucination in large language models. pages 3563–\n",
      "3578.\n",
      "Yann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang,\n",
      "Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy\n",
      "\n",
      "=Contradictory], Reference [2]<p>[ ISREL =Relevant] Rory Tapner was born on 30 September 1959\n",
      "in London, England.[ ISSUP =Fully Supported]\n",
      "Reference [1] Rory Tapner Rory Tapner is a UK businessman who, in September 2010 was appointed CEO of\n",
      "Coutts, the Wealth Management division of Royal Bank of Scotland Group. In February 2015 he stepped down\n",
      "as CEO when he was replaced in the role by Alison Rose\n",
      "Reference [2] Holdings (from August 2016); and the Council of the University of Buckingham (from July 2017).\n",
      "Rory Tapner Rory Tapner (born 30 September 1959) is a UK businessman ...\n",
      "Input Tell me a bio about G. Venugopa. (bio generation)\n",
      "Output (ranked 1) [ Retrieve =Yes] <p>Reference [1]<p>[ ISREL =Relevant] G. Venugopal is a popu-\n",
      "lar playback singer in the Malayalam film industry. [ Retrieve =Continue] He has sung over 300 songs in\n",
      "Malayalam, Tamil, and Hindi movies.[ ISSUP =Partially Supported] [ Retrieve =Yes] <p>Reference\n",
      "\n",
      "and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\n",
      "layer. arXiv preprint arXiv:1701.06538, 2017.\n",
      "[27] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\n",
      "nov. Dropout: a simple way to prevent neural networks from overﬁtting. Journal of Machine\n",
      "Learning Research, 15(1):1929–1958, 2014.\n",
      "[28] Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus. End-to-end memory\n",
      "networks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\n",
      "Advances in Neural Information Processing Systems 28, pages 2440–2448. Curran Associates,\n",
      "Inc., 2015.\n",
      "[29] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\n",
      "networks. In Advances in Neural Information Processing Systems, pages 3104–3112, 2014.\n",
      "[30] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "{\"grade\":\"irrelevant\",\"description\":\"The context provided discusses early Christian observance of the Sabbath and associated historical context, which has no relation to the question about the name of Twitter's CEO. The information presented is entirely unrelated and does not contribute to answering the specific question asked.\"}\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "What is the current name of Twitter's CEO?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  web_search (call_LBy7ru1DE93SvF7iupMvZFvt)\n",
      " Call ID: call_LBy7ru1DE93SvF7iupMvZFvt\n",
      "  Args:\n",
      "    __arg1: current Twitter CEO 2023\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: web_search\n",
      "\n",
      "snippet: In June 2023, Linda Yaccarino, 59, succeeded Elon Musk as the CEO of Twitter, now rebranded as X Corp. Yaccarino says she plans to create a \"Twitter 2.0\" - a trustworthy, accurate and real-time ..., title: Linda Yaccarino - Forbes, link: https://www.forbes.com/profile/linda-yaccarino/, snippet: Twitter, Inc. was an American social media company based in San Francisco, California, which operated and was named for its flagship social media network prior to its rebrand as X.In addition to Twitter, the company previously operated the Vine short video app and Periscope livestreaming service. In April 2023, Twitter merged with X Holdings [6] and ceased to be an independent company ..., title: Twitter, Inc. - Wikipedia, link: https://en.wikipedia.org/wiki/Twitter,_Inc., snippet: The rumors are true: Elon Musk has chosen NBCU leader Linda Yaccarino as the next CEO of Twitter. Musk confirmed Yaccarino's new role in a tweet this morning, a day after he announced that he had ..., title: Elon Musk appoints new Twitter CEO, NBCU's Linda Yaccarino - Yahoo, link: https://www.yahoo.com/news/elon-musk-appoints-twitter-ceo-160114665.html, snippet: Twitter is a real-time microblogging platform, publicly launched in July 2006. At launch, its defining features were the tight limits placed on each post, known at a tweet. Originally, users could only use 140 characters, although that has been elongated over the years. Formed by former Odeo employees Jack Dorsey, Noah Glass, Evan Williams and Biz Stone, the site originally used SMS to send ..., title: Twitter Revenue and Usage Statistics (2025) - Business of Apps, link: https://www.businessofapps.com/data/twitter-statistics/\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "The current CEO of Twitter, which is rebranded as X Corp, is Linda Yaccarino. She succeeded Elon Musk in June 2023.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "The current CEO of Twitter, now rebranded as X Corp, is Linda Yaccarino. She took over the role in June 2023, succeeding Elon Musk.\n"
     ]
    }
   ],
   "source": [
    "user_input = \"what is the name of twitter's ceo?\"\n",
    "\n",
    "initial_state: CRAGState = {\n",
    "    \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": user_input},\n",
    "    ],\n",
    "}\n",
    "\n",
    "events = graph.stream(initial_state, config, stream_mode=\"values\")\n",
    "\n",
    "for event in events:\n",
    "    event[\"messages\"][-1].pretty_print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "what is self RAG?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "indicate the need for retrieval and its generation quality respectively (Figure 1 right). In particular,\n",
      "given an input prompt and preceding generations, SELF -RAG first determines if augmenting the\n",
      "continued generation with retrieved passages would be helpful. If so, it outputs a retrieval token that\n",
      "calls a retriever model on demand (Step 1). Subsequently,SELF -RAG concurrently processes multiple\n",
      "retrieved passages, evaluating their relevance and thengenerating corresponding task outputs (Step\n",
      "2). It then generates critique tokens to criticize its own output and choose best one (Step 3) in terms\n",
      "of factuality and overall quality. This process differs from conventional RAG (Figure 1 left), which\n",
      "1Our code and trained models are available at https://selfrag.github.io/.\n",
      "1\n",
      "arXiv:2310.11511v1  [cs.CL]  17 Oct 2023\n",
      "\n",
      "generation guided by reflections tokens to further improve generation quality and attributions.\n",
      "Concurrent RAG work. A few concurrent works2 on RAG propose new training or prompting\n",
      "strategies to improve widely-adopted RAG approaches. Lin et al. (2023) fine-tune both the retriever\n",
      "and LM on instruction-tuning datasets in two steps. While we also train our model on diverse\n",
      "instruction-following datasets, SELF -RAG enables retrieval on demand and selection of the best\n",
      "possible model output via fine-grained self-reflection, making it widely applicable and more robust\n",
      "and controllable. Yoran et al. (2023) use a natural language inference model and Xu et al. (2023) use\n",
      "a summarization model to filter out or compress retrieved passages before using them to prompt the\n",
      "LM to generate the output. SELF -RAG processes passages in parallel and filters out irrelevant ones\n",
      "through self-reflection, without relying on external models at inference. Moreover, our self-reflection\n",
      "\n",
      "Training overview. SELF -RAG enables an arbitrary LM to generate text with reflection tokens\n",
      "by unifying them as next token predictions from the expanded model vocabulary (i.e., the original\n",
      "vocabulary plus reflection tokens). Specifically, we train the generator model M on a curated corpus\n",
      "with interleaving passages retrieved by a retriever R and reflection tokens predicted by a critic model\n",
      "C (summarized in Appendix Algorithm 2). We train C to generate reflection tokens for evaluating\n",
      "retrieved passages and the quality of a given task output (Section 3.2.1). Using the critic model, we\n",
      "update the training corpus by inserting reflection tokens into task outputs offline. Subsequently, we\n",
      "train the final generator model (M) using the conventional LM objective (Section 3.2.2) to enable\n",
      "M to generate reflection tokens by itself without relying on the critic at inference time.\n",
      "3.2 S ELF -RAG TRAINING\n",
      "\n",
      "SELF -RAG further enables a customizable decoding algorithm to satisfy hard or soft constraints,\n",
      "which are defined by reflection token predictions. In particular, our inference-time algorithm enables\n",
      "us to (1) flexibly adjust retrieval frequency for different downstream applications and (2) customize\n",
      "models’ behaviors to user preferences by leveraging reflection tokens through segment-level beam\n",
      "search using the weighted linear sum of the reflection token probabilities as segment score.\n",
      "Empirical results on six tasks, including reasoning and long-form generation, demonstrate that SELF -\n",
      "RAG significantly outperforms pre-trained and instruction-tuned LLMs that have more parameters and\n",
      "widely adopted RAG approaches with higher citation accuracy. In particular, SELF -RAG outperforms\n",
      "retrieval-augmented ChatGPT on four tasks, Llama2-chat (Touvron et al., 2023) and Alpaca (Dubois\n",
      "et al., 2023) on all tasks. Our analysis demonstrates the effectiveness of training and inference with\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "{\"grade\":\"relevant\",\"description\":\"The context provides a detailed explanation of the concept of SELF-RAG, including its approach to augmenting generation, handling retrieved passages, and utilizing self-reflection to improve output quality. This directly answers the question about what SELF-RAG is.\"}\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "What is the concept of self-Retrieval Augmented Generation (self-RAG)?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  wikipedia_search (call_5s81e6RN671JKceeRfUhxUgF)\n",
      " Call ID: call_5s81e6RN671JKceeRfUhxUgF\n",
      "  Args:\n",
      "    __arg1: Self-Retrieval Augmented Generation\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: wikipedia_search\n",
      "\n",
      "Page: Retrieval-augmented generation\n",
      "Summary: Retrieval-augmented generation (RAG) is a technique that enables generative artificial intelligence (Gen AI) models to retrieve and incorporate new information. It modifies interactions with a large language model (LLM) so that the model responds to user queries with reference to a specified set of documents, using this information to supplement information from its pre-existing training data. This allows LLMs to use domain-specific and/or updated information. Use cases include providing chatbot access to internal company data or generating responses based on authoritative sources.\n",
      "RAG improves large language models (LLMs) by incorporating information retrieval before generating responses. Unlike traditional LLMs that rely on static training data, RAG pulls relevant text from databases, uploaded documents, or web sources. According to Ars Technica, \"RAG is a way of improving LLM performance, in essence by blending the LLM process with a web search or other document look-up process to help LLMs stick to the facts.\" This method helps reduce AI hallucinations, which have led to real-world issues like chatbots inventing policies or lawyers citing nonexistent legal cases.\n",
      "By dynamically retrieving information, RAG enables AI to provide more accurate responses without frequent retraining. According to IBM, \"RAG also reduces the need for users to continuously train the model on new data and update its parameters as circumstances evolve. In this way, RAG can lower the computational and financial costs of running LLM-powered chatbots in an enterprise setting.\"\n",
      "Beyond efficiency gains, RAG also allows LLMs to include source references in their responses, enabling users to verify information by reviewing cited documents or original sources. This can provide greater transparency, as users can cross-check retrieved content to ensure accuracy and relevance.\n",
      "Retrieval-Augmented Generation (RAG) was first introduced in 2020 by Douwe Kiela, Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, and Sebastian Riedel in their research paper Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks, at Meta.\n",
      "\n",
      "\n",
      "\n",
      "Page: Prompt engineering\n",
      "Summary: Prompt engineering is the process of structuring or crafting an instruction in order to produce the best possible output from a generative artificial intelligence (AI) model.\n",
      "A prompt is natural language text describing the task that an AI should perform. A prompt for a text-to-text language model can be a query, a command, or a longer statement including context, instructions, and conversation history. Prompt engineering may involve phrasing a query, specifying a style, choice of words and grammar, providing relevant context, or describing a character for the AI to mimic.\n",
      "When communicating with a text-to-image or a text-to-audio model, a typical prompt is a description of a desired output such as \"a high-quality photo of an astronaut riding a horse\" or \"Lo-fi slow BPM electro chill with organic samples\". Prompting a text-to-image model may involve adding, removing, emphasizing, and re-ordering words to achieve a desired subject, style, layout, lighting, and aesthetic.\n",
      "\n",
      "\n",
      "\n",
      "Page: Recursive self-improvement\n",
      "Summary: Recursive self-improvement (RSI) is a process in which an early or weak artificial general intelligence (AGI) system enhances its own capabilities and intelligence without human intervention, leading to a superintelligence or intelligence explosion.\n",
      "The development of recursive self-improvement raises significant ethical and safety concerns, as such systems may evolve in unforeseen ways and could potentially surpass human control or understanding. There has been a number of proponents that have pushed to pause or slow down AI development for the potential risks of runaway AI systems.\n",
      "\n",
      "\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Self-Retrieval Augmented Generation (self-RAG) is a technique that enhances generative AI models by integrating retrieval processes with generation tasks. This method allows the models to pull relevant information from specified documents or databases during response generation, making their interactions more factual and up-to-date.\n",
      "\n",
      "Key features of self-RAG include:\n",
      "\n",
      "1. **Dynamic Information Retrieval**: Unlike traditional models that rely solely on pre-existing training data, self-RAG retrieves real-time information, improving accuracy and reducing the likelihood of generating false or misleading outputs (often referred to as \"hallucinations\").\n",
      "\n",
      "2. **Improved Performance**: By blending retrieval processes with model generation, self-RAG enables AI to provide more contextually relevant responses without needing continuous retraining on new data.\n",
      "\n",
      "3. **Transparency and Verification**: Self-RAG allows language models to cite their sources, enabling users to verify the information being presented and ensuring greater accountability in responses.\n",
      "\n",
      "This approach not only enhances the efficiency and reliability of responses from AI systems but also lowers computational costs associated with retraining models frequently.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Self-Retrieval Augmented Generation (self-RAG) is a technique that combines retrieval processes with generative AI tasks to improve response accuracy and relevance. It enables models to access real-time information from specific documents, reducing errors and “hallucinations.” Additionally, self-RAG enhances transparency by allowing models to cite sources, making it easier for users to verify information.\n"
     ]
    }
   ],
   "source": [
    "user_input = \"what is self RAG?\"\n",
    "\n",
    "initial_state: CRAGState = {\n",
    "    \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": user_input},\n",
    "    ],\n",
    "}\n",
    "\n",
    "events = graph.stream(initial_state, config, stream_mode=\"values\")\n",
    "\n",
    "for event in events:\n",
    "    event[\"messages\"][-1].pretty_print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
